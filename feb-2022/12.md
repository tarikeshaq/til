# Saturday, Feb 12th

## More and more Swift!
I've been using Swift a decent amount at work, so I finally convinced myself to go through some content on Udemy. I was luckily familiar with the use-cases for a lot of those features, given that they are also features of Rust. Nevertheless, I learned how to use a ton of Swift's features.

### Custom Iterators in Swift!
Implementing a custom iterator in swift is very straight forward - just a matter of
```swift
struct MyIntIterator: IteratorProtocol {
    var num: Int

    mutating func next() -> Int? {
        self.num += 12
        if self.num >= 42 {
            return nil
        }
        return self.num
    }
}
```

The above creates an iterator, `MyIntIterator` that will iterate over increments of 12, until it hits 42 or greater. Ideally, I would also like to have a way to create instances of this iterator from another type... 

```swift
extension Int: Sequence {
    public func makeIterator() -> some IteratorProtocol {
        return MyIntIterator(num: self)
    }
}

```

Then, all we have to do is:
```swift
for num in 0 {
    print(num)
}
```

and we would get

```sh
12
24
36
```

### Lazy sequences and iterators!

So in Swift, the `filter`, `map` and cousins all eagerly evaluate - meaning they will execute right away against the full collection

An example where this isn't great is if you are serving users with data that needs to undergo transformation, and how much of that data the user needs, is determined at runtime.

An example:
```swift
let transformedData = lotsOfData.filter { data in
    return data.isRelevant
}.map { data in
    return transform(data)
}

while let userNeedsDataAt = whatDoesUserWant() {
    show(transformedData[userNeedsDataAt])
}

```

It's a bit convoluted, but the issue above is that we evaluate the filter and the map on **ALL** of `lotsOfData`, even though the user might only need a small subset. lazy sequences and iterators are made exactly for the above problem. 

```swift

let transformedData = lotsOfData.lazy.filter { data in
    return data.isRelevant
}.map { data in
    return transform(data)
}

while let userNeedsDataAt = whatDoesUserWant() {
    show(transformedData[userNeedsDataAt])
}
```

In the above, we will only evaluate what we **need** to evaluate, in order to match the semantics of the earlier version of the code. The meaning of that it a little complicated, since we do a `filter`, we need to make sure to execute the filters correctly so the indexing is correct.. I can probably explain this better, but alas this is not a blog post

### private(set)
This is a neat visibility identifier, pretty much saying that a variable can only be set within the same struct/class/etc. It works the same way with `internal(set)`, `fileprivate(set)`, etc!
```swift
struct MyStruct {
    public private(set) var prop: Int
}
```

## Designing Data Intensive Application
I'm re-reading an amazing book, [Designing Data Intensive Applications](https://dataintensive.net/) - I'm currently at chapter 9 (I previously read through until chapter 11)

### Linearizability
This chapter is all about consistency and guarantees, so it first introduces Linearizability as the strongest consistency guarantee you can make. Linearizability in short, is the system behaving **as-if** there exists exactly one copy of the data. In practice, this means that once **any** client has seen a write, all other clients will now see that updated write. 

For example, assume there are three clients, and that `x`s initial value is `0`
- Client A: Wants to write(x) = 42
- Client B: Wants to read(x)
- Client C: Wants to read(x)
In a Linearizable system, if client `B`'s read happens before client `C`, and client `B` sees `x = 42` - then client `C` must see `x = 42` as well. This is regardless of which nodes of the distributed system the clients are reading from.

My own reflection as I was reading this, and later re-assured by the author: there is limited use cases that **require** a guarantee this strong, and it seems very problematic to implement

#### Linearizability vs Serializability
Two chapters ago, I learned about Serializability: It's the system behaving as if all transactions were executed in **some** sequential order. 

Linearizability does not prevent write skews on its own.

The author explains the difference, and how neither would guarantee the other (although some implementations of Serializability gaurentee Linearizability, but not all)

#### Use cases of Linearizability
There are a few use cases where Linearizability can be a a great guarantee:
- Leader election in single-leader replication: All nodes must agree on who is the leader, distributed locking is sometimes used to do this
- Uniqueness guarantees and Constraints: If an application has strong constraints, for example if bank account value cannot be negative, etc. Although some of those can sometimes be dealt with (i.e in the contract allow a buffer of how negative the account can get)
- Cross-channel timing dependencies: If there are other communication channels that might cause linearizability violations to be caught (for example, users talking to each other about the values they got from a query)

#### Implementations of Linearizability
Depending on the replication model, the system may or may not be linearizable and the **how** will be different:
- Single leader replication: Potentially linearizable if reads are done from the leader and/or sequentially updated followers
- multi-leader replication: by definition non-linearizable. You can't have the system act as if there exists one copy of that data, when a key feature of your replication is that there multiple acting independently
- Leaderless replication: Probably not linearizable, even if we have quorms, network delays can cause violations. It is **possible** to linearize by making sure that nodes apply a synchronous Read-repair on each query... although that's a strong hit on performance

#### Cost of Linearizability
There has been a proof that the performance cost of linerizability is at least proportional to the uncertainty in network delay. Additionally, there will **always** be a cost on availability if a network partition occurs.
